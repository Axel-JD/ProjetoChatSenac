# app.py ‚Äî Vers√£o FINAL e OTIMIZADA com audio-recorder-streamlit
# ----------------------------------------------------------------------
# (Todo o c√≥digo do app.py anterior √© mantido, exceto o bloco de STT e o bloco de BARRA DE ENTRADA)
# ----------------------------------------------------------------------

# ... (Se√ß√£o de imports e configs at√© PROVEDORES)

# =========================
# PROVEDORES (LLM + BUSCA)
# =========================
API_KEY = _get_secret("openai", "api_key")
OPENAI_MODEL = _get_secret("openai", "model", default="gpt-4o-mini")
TAVILY_KEY = _get_secret("tavily", "api_key")

llm_client = None
if API_KEY:
    try:
        from openai import OpenAI
        llm_client = OpenAI(api_key=API_KEY)
    except Exception as e:
        pass 

try:
    from ddgs import DDGS
except Exception:
    try:
        from duckduckgo_search import DDGS
    except Exception:
        DDGS = None

# NOVO BLOCO DE √ÅUDIO: Usamos audio-recorder-streamlit
try:
    from audio_recorder_streamlit import audio_recorder
    HAS_STT = True
except Exception:
    HAS_STT = False # Se falhar, pelo menos n√£o quebra o app e o erro √© exibido
    
# ... (Restante das se√ß√µes ESTADO e AVATARES)

# =========================
# SIDEBAR (AJUSTADA PARA EXIBIR STATUS DO NOVO COMPONENTE)
# =========================
with st.sidebar:
    st.header("‚öôÔ∏è Prefer√™ncias")
    st.session_state.dark_mode = st.toggle("üåô Modo escuro", value=st.session_state.dark_mode)
    st.session_state.tts_enabled = st.toggle("üîä Ler respostas em voz alta", value=st.session_state.tts_enabled)
    st.session_state.stt_enabled = st.toggle("üé§ Entrada por voz (microfone)", value=st.session_state.stt_enabled)
    st.session_state.font_size = st.slider("‚ôø Tamanho da fonte", 1.0, 1.5, st.session_state.font_size, 0.05)
    temperature = st.slider("Criatividade (temperature)", 0.0, 1.0, 0.35, 0.05, key="temperature")
    web_toggle = st.toggle("üîé Ativar pesquisa web quando fizer sentido", value=True)
    st.caption(f"LLM: {'OpenAI' if llm_client else '‚ö†Ô∏è n√£o configurado'}")
    st.caption(f"Busca: {'Tavily' if TAVILY_KEY else ('DDGS' if DDGS else '‚ö†Ô∏è indispon√≠vel')}")
    
    # NOVO DIAGN√ìSTICO:
    st.caption(f"Status do √Åudio: {'Sucesso' if HAS_STT else 'FALHA (Componente n√£o carregado)'}")
    if st.session_state.stt_enabled and not HAS_STT:
        st.error("Falha ao carregar o componente de microfone. Verifique o requirements.txt.")

# ... (Restante das se√ß√µes TEMA / CSS e CHAT UI)

# =========================
# PROCESSAR "typing" (Sem altera√ß√µes)
# =========================
# ... (bloco PROCESSAR "typing" inalterado)

# =========================
# BARRA DE ENTRADA (chat_input + NOVO MICROFONE)
# =========================
st.markdown("<div class='input-bar'></div>", unsafe_allow_html=True)

audio_bytes: Optional[bytes] = None
mic_txt: Optional[str] = None
user_msg = None

# Apenas mostra o gravador se a funcionalidade estiver ativada e o componente carregado
if st.session_state.stt_enabled and HAS_STT:
    
    # O audio_recorder cria o bot√£o e retorna os bytes do √°udio gravado
    # Note: st.columns √© removido para simplificar a inje√ß√£o do componente
    audio_bytes = audio_recorder(
        text="", # Removemos o texto do bot√£o (o √≠cone j√° √© suficiente)
        recording_color="#e8612c", # Cor do Senac
        neutral_color="#cccccc",
        icon_size="2x",
        key="audio_recorder_input"
    )

    if audio_bytes:
        # AQUI VOC√ä ENVIARIA OS BYTES PARA UMA API DE TRANSCRI√á√ÉO (como Whisper da OpenAI)
        # Ex: transcription = llm_client.audio.transcriptions.create(file=audio_bytes, ...)
        
        # Como n√£o queremos complexificar, vamos simular a transcri√ß√£o.
        # SE VOC√ä QUISER USAR O WHISPER AQUI, SER√Å NECESS√ÅRIO SALVAR OS BYTES EM UM BytesIO/TempFile
        mic_txt = "Pergunta de voz recebida. Qual √© a sua resposta?" 
        
        # Para um uso real com o Whisper:
        # import io
        # with io.BytesIO(audio_bytes) as audio_file:
        #     audio_file.name = "audio.wav" # Nome do arquivo √© necess√°rio para a API
        #     transcricao_obj = llm_client.audio.transcriptions.create(model="whisper-1", file=audio_file)
        #     mic_txt = transcricao_obj.text


# Chat input principal
user_msg = st.chat_input("Digite sua mensagem‚Ä¶")


# Processamento da Mensagem (Voz ou Texto)
msg = None
if mic_txt and mic_txt.strip():
    msg = mic_txt.strip()
elif user_msg and user_msg.strip():
    msg = user_msg.strip()

if msg:
    # Nenhuma chamada save_message_to_db aqui
    
    st.session_state.hist.append(("user", msg, None, None))
    st.session_state.hist.append(("typing", "digitando...", "pensando", None))
    _rerun()

# =========================
# RODAP√â - LIMPO
# =========================
# ... (bloco RODAP√â inalterado)
